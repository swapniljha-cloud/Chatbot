import json
import re
import tesorflow as tf

# Loading Data From Json File
file = open("C:\\Users\\user\\Desktop\\CO600\\qaPairs.json").read()
data = json.loads(file)["data"]

paragraph = []
qes_data = []
ans_data = []
unanswered_ques = []

# Obtaining Q & A Data from qes dictionary within Json File. Each qes dictionary has all question and associated answer.

for qaPairs in data:
    for paragraph in qaPairs["paragraphs"]:
        for qaPair in paragraph["qas"]:
            if qaPair["answers"]:
                qes_data.append((qaPair['question']))
                ans_data.append((qaPair["answers"][0]["text"]))
            else:
                unanswered_ques.append(qaPair["question"])


# Function to clean the text
def clean_text(text):
    text = text.lower()
    text = re.sub(r"i'm", "i am", text)
    text = re.sub(r"he's", "he is", text)
    text = re.sub(r"she's", "she is", text)
    text = re.sub(r"that's", "that is", text)
    text = re.sub(r"what's", "what is", text)
    text = re.sub(r"where's", "where is", text)
    text = re.sub(r"\'ll", " will", text)
    text = re.sub(r"it's", "it is", text)
    text = re.sub(r"\'re", " are", text)
    text = re.sub(r"\'d", " would", text)
    text = re.sub(r"\'ve", " have", text)
    text = re.sub(r"won't", "will not", text)
    text = re.sub(r"can't", "can not", text)
    text = re.sub(r"[-()\"#/@;:{}=+~|.?,]", "", text)
    return text


# Cleaning all the questions and answers
clean_question_data = []
clean_answer_data = []

for question in qes_data:
    clean_question_data.append(clean_text(question))

for answer in ans_data:
    clean_answer_data.append(clean_text(answer))

# Count the number of words in the data
word_to_count = {}
for q_data in clean_question_data:
    for word in q_data.split():
        if word not in word_to_count:
            word_to_count[word] = 1
        else:
            word_to_count[word] += 1

for a_data in clean_answer_data:
    for word in a_data.split():
        if word not in word_to_count:
            word_to_count[word] = 1
        else:
            word_to_count[word] += 1

# Creating two dictionaries that map the questions words and the answers word to a unique integer
threshold_questions = 20
questionswords2int = {}
word_ref = 0
for word, count in word_to_count.items():
    if count >= threshold_questions:
        questionswords2int[word] = word_ref
        word_ref += 1

threshold_answers = 20
answerswords2int = {}
word_ref = 0
for word, count in word_to_count.items():
    if count >= threshold_answers:
        answerswords2int[word] = word_ref
        word_ref += 1

# Adding the last tokens to these 2 dictionaries

tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']
for token in tokens:
    questionswords2int[token] = len(questionswords2int) + 1
    answerswords2int[token] = len(answerswords2int) + 1

# Creating the inverse dictionary of the answerswords2int dictionary
ans_ints_to_word = {w_i: w for w, w_i in answerswords2int.items()}

qes_ints_to_word = {w_i: w for w, w_i in questionswords2int.items()}

# Adding EOS tokens to the end of each sentence

for i in range(len(clean_question_data)):
    clean_question_data[i] += ' <EOS>'

for i in range(len(clean_answer_data)):
    clean_answer_data[i] += ' <EOS>'

# Translating all the questions and the answers into integers and replacing
# all the words that were filtered out by out
qes_into_int = []
for question in clean_question_data:
    ints = []
    for word in question.split():
        if word not in questionswords2int:
            ints.append(questionswords2int['<OUT>'])
        else:
            ints.append(questionswords2int[word])
    qes_into_int.append(ints)
ans_into_int = []
for answer in clean_answer_data:
    ints = []
    for word in answer.split():
        if word not in answerswords2int:
            ints.append(answerswords2int['<OUT>'])
        else:
            ints.append(answerswords2int[word])
    ans_into_int.append(ints)

# Sorting questions and answers by the length of questions
sorted_clean_qes = []
sorted_clean_ans = []
for length in range(1, 25 + 1):
    for i in enumerate(qes_into_int):
        if len(i[1]) == length:
            sorted_clean_qes.append(qes_into_int[i[0]])
            sorted_clean_ans.append(ans_into_int[i[0]])


# Creating placeholders for the inputs and the targets

def model_inputs():
    inputs = tf.placeholder(tf.int32, [None, None], name='input')
    targets = tf.placeholder(tf.int32, [None, None], name='target')
    lr = tf.placeholder(tf.float32, name='learning_rate')
    keep_prob = tf.placeholder(tf.float32, name='keep_prob')
    return inputs, targets, lr, keep_prob


# Preprocessing the target

def preprocess_targets(targets, word2int, batch_size):
    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])
    right_side = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])
    preprocessed_targets = tf.concat([left_side, right_side], 1)
    return preprocessed_targets


# Creating the encoder RNN layer

def encoder_rnn_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):
    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)
    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)
    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)
    _, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,
                                                       cell_bw=encoder_cell,
                                                       sequence_length=sequence_length,
                                                       inputs=rnn_inputs,
                                                       dtype=tf.float32)
    return encoder_state
